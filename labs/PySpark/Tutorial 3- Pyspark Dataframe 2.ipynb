{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded0783b",
   "metadata": {},
   "source": [
    "### Pyspark Dataframes continued, handling missing values, SQL library\n",
    "\n",
    "Jay Urbain, PhD   \n",
    "3/15/2024\n",
    "\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter In Dropping functionalities\n",
    "- Handling Missing values by Mean, MEdian And Mode\n",
    "- Updating a dataframe\n",
    "\n",
    "PySpark documentation:   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/index.html \n",
    "\n",
    "Answer the TODO items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f922300-fe03-4bcf-b2c8-c0f98806f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d82dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "\n",
    "import os\n",
    "\n",
    "JAVA_HOME = \"/home/jay/anaconda3/envs/pyspark\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "\n",
    "# option 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/jayurbain/opt/miniconda3/envs/pyspark\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed677b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name column\n",
    "\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c041e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36845e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any==how\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold\n",
    "\n",
    "df_pyspark.na.drop(how=\"any\",thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use subset for selecting rows\n",
    "\n",
    "df_pyspark.na.drop(how=\"any\",subset=['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bad9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filling the Missing Value\n",
    "df_pyspark.na.fill('Missing Values',['Experience','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e01bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8ad3f",
   "metadata": {},
   "source": [
    "TODO: What's goig on here? I thought we removed those rows. Explain here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66832fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31190f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e3e46",
   "metadata": {},
   "source": [
    "Update Spark Dataframe\n",
    "\n",
    "You can do an update on a PySpark DataFrame Column using withColum(), select() and sql(), since DataFrame’s are distributed immutable collection you can’t really change the column values however when you change the value using withColumn() or any approach, PySpark returns a new Dataframe with updated values. \n",
    "\n",
    "Read data into dataframe from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a3183-0f2f-4c8b-b01b-4fb7bb15cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000), ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200)\n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f5a17",
   "metadata": {},
   "source": [
    "Below PySpark code updates the salary column value of DataFrame by multiplying salary by 3 times. \n",
    "\n",
    "Note that withColumn() is used to update or add a new column to the DataFrame, when you pass the existing column name to the first argument to withColumn() operation it updates it , if the value is new then it creates a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.withColumn(\"salary\", df.salary*3)\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f080c",
   "metadata": {},
   "source": [
    "Update Column Based on Condition\n",
    "\n",
    "Update a column value based on a condition by using `When Otherwise`. The example below updates the gender column with a value Male for M, Female for F and keeps the same value for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df3 = df.withColumn(\"gender\", when(df.gender == \"M\",\"Male\") \\\n",
    "      .when(df.gender == \"F\",\"Female\") \\\n",
    "      .otherwise(df.gender))\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647a62f",
   "metadata": {},
   "source": [
    "Update DataFrame Column Data Type\n",
    "\n",
    "You can also update a Data Type of a column using withColumn(). In addition you have to use the cast() function of the PySpark Column class. The code below updates salary column to String type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c772cb",
   "metadata": {},
   "source": [
    "PySpark SQL Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"PER\")\n",
    "df5=spark.sql(\"select firstname,gender,salary*3 as salary from PER\")\n",
    "df5.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674d92d-0f4e-4b2d-a889-667afe670e58",
   "metadata": {},
   "source": [
    "#### TODO: Use spark.sql to order the original dataframe by salary descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc83f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10500df0-5bbd-49c9-a6d1-d6fba5765181",
   "metadata": {},
   "source": [
    "#### TODO: Use spark.sql to determine the average salary for each gender, order results by average salary descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cf17b-edbd-43fc-ae1a-5d58ec70bad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d84ad-c8e4-4008-87a6-15d347bbe2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3070b-845d-4262-b6e6-73589bbdc813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pysparek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
