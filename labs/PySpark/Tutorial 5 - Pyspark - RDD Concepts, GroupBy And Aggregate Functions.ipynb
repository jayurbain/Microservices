{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa0fce",
   "metadata": {},
   "source": [
    "### Pyspark - RDD Concepts, GroupBy And Aggregate Functions\n",
    "\n",
    "Jay Urbain, PhD   \n",
    "3/18/2024\n",
    "\n",
    "\n",
    "PySpark documentation:   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/index.html  \n",
    "\n",
    "Answer the TODO items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474b95e",
   "metadata": {},
   "source": [
    "![](spark-cluster-overview.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b8e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "\n",
    "import os\n",
    "\n",
    "JAVA_HOME = \"/home/jay/anaconda3/envs/pyspark\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "\n",
    "# option 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/jayurbain/opt/miniconda3/envs/pyspark\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f336300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3de8b",
   "metadata": {},
   "source": [
    "In order to create an RDD, first, you need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. You can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23513a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 18:57:24 WARN Utils: Your hostname, tensorbook resolves to a loopback address: 127.0.1.1; using 192.168.86.36 instead (on interface wlp0s20f3)\n",
      "24/03/20 18:57:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/20 18:57:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/20 18:57:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/20 18:57:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Agg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248b3a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://tensorbook.lan:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Agg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f819ae2b590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308375d",
   "metadata": {},
   "source": [
    "#### PySpark RDD – Resilient Distributed Dataset\n",
    "\n",
    "PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
    "\n",
    "In order to create an RDD, we need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "\n",
    "Spark session internally creates a sparkContext variable of SparkContext. We can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one.\n",
    "\n",
    "If you have tabular data, you should just use a DataFrame. Going down to the RDD layer is helpful for unstructured data, for example, text, images, and signal data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bfc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from list using parallelize\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3db85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from external data source\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd2=spark.sparkContext.textFile('test3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed6295",
   "metadata": {},
   "source": [
    "#### RDD Operations\n",
    "\n",
    "On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "RDD transformations – Transformations are lazy operations. When you run a transformation(for example update), instead of updating a current RDD, these operations return another RDD.\n",
    "\n",
    "RDD actions – operations that trigger computation and return RDD values to the driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf741fb",
   "metadata": {},
   "source": [
    "#### RDD Transformations\n",
    "\n",
    "Transformations on Spark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aef9a9",
   "metadata": {},
   "source": [
    "#### PySpark DataFrame\n",
    "\n",
    "DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with  optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7b537",
   "metadata": {},
   "source": [
    "#### TODO: Is PySpark faster than pandas? Explain your thinking.**\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11432d7",
   "metadata": {},
   "source": [
    "Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed791ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405ebe6",
   "metadata": {},
   "source": [
    "PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.\n",
    "\n",
    "In other words, Spark SQL brings native SQL queries on Spark meaning you can run traditional ANSI SQL’s on Spark Dataframe, in the later section of this PySpark SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
    "\n",
    "In order to use SQL, first, create a temporary table on DataFrame using createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
    "\n",
    "Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed27207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.createOrReplaceTempView(\"PEOPLE\")\n",
    "df1=spark.sql(\"select Name, Departments, Salary as salary from PEOPLE\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupby\n",
    "\n",
    "# Grouped to find the maximum salary\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the operation above using PySpark SQL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc122ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Use PySpark dataframe to group by departmebnnts and compute sum for each department\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc856adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Use PySpark SQL to group by departmebnnts and compute sum for each department\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Use PySpark dataframe to group and compute count for each department\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb50fa0",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b26cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21f03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-2",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
