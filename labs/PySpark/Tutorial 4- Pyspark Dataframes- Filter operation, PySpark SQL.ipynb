{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6364f6",
   "metadata": {},
   "source": [
    "### Pyspark Dataframes - Filter Operations, PySpark SQL\n",
    "\n",
    "3/18/2024\n",
    "\n",
    "Answer the TODO items.\n",
    "- Filter Operation\n",
    "- &,|,==\n",
    "- \n",
    "- ~\n",
    "\n",
    "PySpark documentation:   \n",
    "https://spark.apache.org/docs/latest/api/python/reference/index.html  \n",
    "\n",
    "Answer the TODO items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "\n",
    "import os\n",
    "\n",
    "JAVA_HOME = \"/home/jay/anaconda3/envs/pyspark\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "\n",
    "# option 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/jayurbain/opt/miniconda3/envs/pyspark\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c2809",
   "metadata": {},
   "source": [
    "PySpark SQL\n",
    "\n",
    "PySpark SQL is one of the most used PySpark modules which is used for processing structured columnar data format. Once you have a DataFrame created, you can interact with the data by using SQL syntax.\n",
    "\n",
    "In other words, Spark SQL brings native SQL queries on Spark meaning you can run traditional ANSI SQLâ€™s on Spark Dataframe, in the later section of this PySpark SQL tutorial, you will learn in detail using SQL select, where, group by, join, union e.t.c\n",
    "\n",
    "In order to use SQL, first, create a temporary table on DataFrame using createOrReplaceTempView() function. Once created, this table can be accessed throughout the SparkSession using sql() and it will be dropped along with your SparkContext termination.\n",
    "\n",
    "Use sql() method of the SparkSession object to run the query and this method returns a new DataFrame.\n",
    "\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.createOrReplaceTempView(\"PEOPLE\")\n",
    "df1=spark.sql(\"select Name, age, Experience, Salary as salary from PEOPLE\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdbb15",
   "metadata": {},
   "source": [
    "### Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21edffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Salary of the people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the equivalent operation using PySpark SQL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(\"Salary<20000\").select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the equivalent operation (above) using PySpark SQL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebe963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<20000) | \n",
    "                  (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9af0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the same operation (above) except use logical AND using PySpark Dataframe \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b356a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the same operation  (above) logical AND and select pnly the Name and Salary using PySpark SQL \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc87f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: Perform the same operation (above) using PySpark Dataframe \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4375a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pysparek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
